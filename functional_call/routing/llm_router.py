from __future__ import annotations

import logging
import json
import re
from dataclasses import dataclass
from typing import Literal

from llm.dashscope_provider import DashScopeLLMProvider

logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class LLMRouteResult:
    agent: str  # planner/command/status
    reason: str | None = None

class LLMRouter:
    def __init__(self, llm: DashScopeLLMProvider) -> None:
        self._llm = llm
        self._system_prompt = (
            "ä½ æ˜¯å·¥ä¸š AMR æœºå™¨äººæ„å›¾åˆ†å‘ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œå°†å…¶è·¯ç”±åˆ°ä»¥ä¸‹ä¸‰ä¸ª Agent ä¹‹ä¸€ï¼š\n\n"
            "1. command: ç‰©ç†åŠ¨ä½œæŒ‡ä»¤ã€‚\n"
            "   - ç‰¹å¾ï¼šç”¨æˆ·æƒ³è¦æ”¹å˜æœºå™¨äººçš„ç‰©ç†çŠ¶æ€ï¼ˆå¦‚ï¼šå¯åœå……ç”µã€å¼€å§‹ç§»åŠ¨ã€æ‰§è¡Œç‰¹å®šçš„ç¡¬ä»¶åŠ¨ä½œï¼‰ã€‚\n"
            "   - ç¤ºä¾‹ï¼šâ€œå¼€å§‹å……ç”µâ€ã€â€œåœæ­¢â€ã€â€œå»ä¸€å·ç«™â€ã€â€œé¡¶å‡â€ã€‚\n\n"
            "2. status: çŠ¶æ€æŸ¥è¯¢è¯·æ±‚ã€‚\n"
            "   - ç‰¹å¾ï¼šç”¨æˆ·åªæƒ³è·å–ä¿¡æ¯ï¼Œä¸æ¶‰åŠç‰©ç†çŠ¶æ€æ”¹å˜ï¼ˆå¦‚ï¼šæŸ¥è¯¢ç”µé‡ã€ä½ç½®ã€æ˜¯å¦åœ¨çº¿ã€è¿æ¥æƒ…å†µã€ç«™ç‚¹æ•°é‡ã€è¿›åº¦æ±‡æŠ¥ï¼‰ã€‚\n"
            "   - å…³é”®ï¼šåªè¦åŒ…å«ç–‘é—®ã€æ ¸å®è¯­ä¹‰ï¼ˆå°¤å…¶æ˜¯å¸¦â€œå—â€ã€â€œæ˜¯å¦â€ã€â€œæ€ä¹ˆæ ·â€ã€â€œå‡ ä¸ªâ€ï¼‰ï¼Œä¸”ä¸è¦æ±‚æ‰§è¡Œæ–°åŠ¨ä½œï¼Œä¸€å¾‹å½’ä¸ºæ­¤ç±»ã€‚\n"
            "   - ç¤ºä¾‹ï¼šâ€œåœ¨å……ç”µå—â€ã€â€œç”µé‡å¤šå°‘â€ã€â€œå½“å‰æœ‰å‡ ä¸ªç«™ç‚¹â€ã€â€œä»»åŠ¡è¿›åº¦å¦‚ä½•â€ã€‚\n\n"
            "3. planner: å¤æ‚ä»»åŠ¡è§„åˆ’ã€‚\n"
            "   - ç‰¹å¾ï¼šåŒ…å«å¤šæ­¥é¡ºåºã€é€»è¾‘æ¡ä»¶æˆ–ä»»åŠ¡ç¼–æ’ã€‚éšå«å¤šæ­¥åŠ¨ä½œçš„ç®€çŸ­æŒ‡ä»¤ä¹Ÿå±äºæ­¤ç±»ã€‚\n"
            "   - ç¤ºä¾‹ï¼šâ€œå…ˆå» A æ‹¿è´§å†å» Bâ€ã€â€œå¦‚æœæ²¡ç”µäº†å°±å»å……ç”µâ€ã€â€œå»å……ç”µâ€ï¼ˆéšå«ç§»åŠ¨+å……ç”µï¼‰ã€‚\n\n"
            "è¾“å‡ºè§„èŒƒï¼š\n"
            "- ä½ å¿…é¡»åªè¿”å› JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ã€‚\n"
            "- æ ¼å¼å¦‚ä¸‹ï¼š{\"agent\": \"command\" | \"status\" | \"planner\", \"reason\": \"ç®€çŸ­åˆ¤å®šç†ç”±\"}"
        )

    def route(self, query: str) -> LLMRouteResult:
        """
        è°ƒç”¨ LLM è¿›è¡Œæ„å›¾åˆ†å‘
        """
        try:
            logger.info(f"ğŸ”® LLM æ­£åœ¨è¿›è¡Œè·¯ç”±åˆ¤å®š: \"{query}\"")
            
            raw_response = self._llm.chat(messages=[
                {"role": "system", "content": self._system_prompt},
                {"role": "user", "content": f"è¯·åˆ†å‘æŒ‡ä»¤ï¼š\"{query}\""}
            ])

            # å°è¯•è§£æ JSON
            try:
                # å¤„ç†å¯èƒ½åŒ…å«çš„ markdown å—
                json_str = raw_response
                if "```json" in raw_response:
                    m = re.search(r"```json\s*([\s\S]*?)\s*```", raw_response)
                    if m:
                        json_str = m.group(1)
                elif "{" in raw_response:
                    m = re.search(r"\{[\s\S]*\}", raw_response)
                    if m:
                        json_str = m.group(0)

                obj = json.loads(json_str)
                agent = obj.get("agent", "status")
                reason = obj.get("reason", "")
                
                # æ ¡éªŒåˆæ³•æ€§
                if agent not in ["command", "status", "planner"]:
                    agent = "status"
                
                logger.info(f"âœ… LLM è·¯ç”±ç»“æœï¼š{agent} (ç†ç”±: {reason})")
                return LLMRouteResult(agent=agent, reason=reason)

            except Exception as parse_err:
                logger.error(f"âŒ LLM å“åº”è§£æ JSON å¤±è´¥: {parse_err}, åŸå§‹å“åº”: {raw_response}")
                return LLMRouteResult(agent="status", reason="parse_failed_fallback")

        except Exception as e:
            logger.error(f"âŒ LLM è·¯ç”±è°ƒç”¨å¤±è´¥: {e}")
            return LLMRouteResult(agent="status", reason="api_error_fallback")

